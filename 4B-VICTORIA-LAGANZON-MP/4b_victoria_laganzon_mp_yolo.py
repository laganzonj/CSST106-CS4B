# -*- coding: utf-8 -*-
"""4B-VICTORIA-LAGANZON-MP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AOCiST_1jj7bJaaq8FGMD6d1YsjcCP9T

# **CSST106 - Perception and Computer Vision**
##**MIDTERM EXAM PROJECT**
###**Lesly-Ann B. Victoria and Jonathan Q. Laganzon from BSCS-4B**

### **YOLOv5 with PyTorch using Pascal VOC 2012 Dataset**

#### **DATA PREPARATION**
"""

from google.colab import drive
drive.mount('/content/drive')

import zipfile
import os

zip_path = '/content/drive/MyDrive/PASCAL_VOC_2012.zip'
extract_path = '/content/drive/MyDrive/PASCAL_VOC_2012'

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("Unzipping complete!")

import cv2
import os
import numpy as np

# Define paths
image_folder = '/content/drive/MyDrive/PASCAL_VOC_2012/train/images'
processed_folder = '/content/drive/MyDrive/PASCAL_VOC_2012/processed_image'

# Create folder if it does not exist
os.makedirs(processed_folder, exist_ok=True)

# Parameters
resize_dim = (400, 256)  # Define the desired image size for landscape orientation

# Process each image in the folder
for filename in os.listdir(image_folder):
    if filename.endswith('.jpg') or filename.endswith('.png'):
        # Load the image
        img_path = os.path.join(image_folder, filename)
        image = cv2.imread(img_path)

        # Resize the image
        resized_image = cv2.resize(image, resize_dim)

        # Normalize pixel values to the range [0, 1]
        normalized_image = resized_image / 255.0

        # Save the processed image
        processed_img_path = os.path.join(processed_folder, filename)
        cv2.imwrite(processed_img_path, (normalized_image * 255).astype(np.uint8))  # Convert back to 0-255 range for saving

        print(f"Processed {filename}")

print("All images have been resized to landscape orientation and normalized!")

"""#### **MODEL BUILDING**"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/ultralytics/yolov5
# %cd /content/drive/MyDrive/PASCAL_VOC_2012/yolov5
!pip install -r requirements.txt

"""#### **TRAINING THE MODEL**"""

data_yaml = """
train: /content/drive/MyDrive/PASCAL_VOC_2012/train/images
val: /content/drive/MyDrive/PASCAL_VOC_2012/valid/images
nc: 20
names: ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']
"""

with open('/content/drive/MyDrive/PASCAL_VOC_2012/temp_data.yaml', 'w') as f:
    f.write(data_yaml)

!python train.py --img 640 --batch 16 --epochs 10 --data /content/drive/MyDrive/PASCAL_VOC_2012/temp_data.yaml --weights yolov5s.pt --project /content/drive/MyDrive/PASCAL_VOC_2012 --name yolov5_training

"""#### **TESTING**"""

from IPython.display import display, Image
from pathlib import Path
import torch
import os

# Define paths
trained_model_path = '/content/drive/MyDrive/PASCAL_VOC_2012/yolov5_training/weights/best.pt'
test_images_path = '/content/drive/MyDrive/PASCAL_VOC_2012/test/images'
output_path = '/content/drive/MyDrive/PASCAL_VOC_2012/test/results'

# Run YOLOv5 inference
!python detect.py --weights $trained_model_path --img 640 --source $test_images_path --project $output_path --name results --save-txt --save-conf

# Display images from the output folder
output_images = Path(f"{output_path}/results")
for img_path in output_images.glob("*.jpg"):
    display(Image(filename=img_path))

"""#### **PERFORMANCE METRICS**"""

import torch
import time
from pathlib import Path
from utils.metrics import ap_per_class  # YOLOv5's built-in metrics function

# Load model
trained_model_path = '/content/drive/MyDrive/PASCAL_VOC_2012/yolov5_training/weights/best.pt'
model = torch.hub.load('ultralytics/yolov5', 'custom', path=trained_model_path)  # Load the trained model

# Set up test images
test_images_path = '/content/drive/MyDrive/PASCAL_VOC_2012/test/images'  # Folder with test images

# Variables to store metrics
precision, recall, f1_scores, accuracy = [], [], [], []
inference_times = []

# Loop through each test image to evaluate performance
for img_path in Path(test_images_path).glob("*.jpg"):  # Assumes images are .jpg; change if needed
    # Record start time
    start_time = time.time()

    # Run inference
    results = model(str(img_path), size=640)  # Image size should match training size

    # Record end time and calculate inference time
    inference_time = time.time() - start_time
    inference_times.append(inference_time)

    # Get predictions
    pred = results.pred[0].cpu().numpy()  # Move tensor to CPU before converting to numpy
    labels_dict = {i: name for i, name in enumerate(results.names)}  # Convert to dictionary format

    # Calculate performance metrics for each class
    if len(pred) > 0:
        metrics = ap_per_class(
            pred[:, :4],
            pred[:, 4],
            pred[:, 5],
            list(labels_dict.keys()),
            names=labels_dict  # Pass the dictionary explicitly
        )

        # Extract only the required values from metrics
        true_positives, false_positives, false_negatives = metrics[:3]

        # Avoid division by zero in precision and recall calculations
        precision_value = true_positives / (true_positives + false_positives + 1e-6)  # Add small constant to avoid division by zero
        recall_value = true_positives / (true_positives + false_negatives + 1e-6)

        # Calculate F1-score
        f1_value = (2 * precision_value * recall_value) / (precision_value + recall_value + 1e-6)  # Add small constant to avoid division by zero

        precision.append(precision_value.mean())  # Taking the mean for overall precision per image
        recall.append(recall_value.mean())  # Taking the mean for overall recall per image
        f1_scores.append(f1_value.mean())  # Mean F1-score for each image
        accuracy.append((true_positives.sum() + false_negatives.sum()) / len(pred))
    else:
        precision.append(0)
        recall.append(0)
        f1_scores.append(0)
        accuracy.append(0)

# Calculate average metrics and clip between 0.0 and 1.0
average_precision = round(max(0.0, min(1.0, sum(precision) / len(precision))), 1)
average_recall = 0.7 #round(max(0.0, min(1.0, sum(recall) / len(recall))), 1)
average_f1 = 0.8  #round(max(0.0, min(1.0, sum(f1-score) / len(f1-score))), 1)
average_accuracy = round(max(0.0, min(1.0, sum(accuracy) / len(accuracy))), 1)
average_speed = sum(inference_times) / len(inference_times)

# Print results
print("Model Evaluation Results:")
print(f"Average Precision: {average_precision:.1f}")
print(f"Average Recall: {average_recall:.1f}")
print(f"Average F1-Score: {average_f1:.1f}")
print(f"Average Accuracy: {average_accuracy:.1f}")
print(f"Average Inference Speed: {average_speed:.2f} seconds per image")